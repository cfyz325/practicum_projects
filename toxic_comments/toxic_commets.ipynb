{
  "metadata": {
    "ExecuteTimeLog": [
      {
        "duration": 5420,
        "start_time": "2023-05-15T09:42:04.010Z"
      },
      {
        "duration": 2489,
        "start_time": "2023-05-15T09:42:39.336Z"
      },
      {
        "duration": 22,
        "start_time": "2023-05-15T09:42:50.706Z"
      },
      {
        "duration": 24,
        "start_time": "2023-05-15T09:43:04.168Z"
      },
      {
        "duration": 37,
        "start_time": "2023-05-15T09:43:11.074Z"
      },
      {
        "duration": 3682,
        "start_time": "2023-05-15T09:46:55.526Z"
      },
      {
        "duration": 356,
        "start_time": "2023-05-15T09:47:36.580Z"
      },
      {
        "duration": 482,
        "start_time": "2023-05-15T09:49:05.517Z"
      },
      {
        "duration": 43,
        "start_time": "2023-05-15T09:49:40.622Z"
      },
      {
        "duration": 1425,
        "start_time": "2023-05-15T09:49:52.294Z"
      },
      {
        "duration": 1639,
        "start_time": "2023-05-15T11:38:40.469Z"
      },
      {
        "duration": 385,
        "start_time": "2023-05-15T11:46:29.562Z"
      },
      {
        "duration": 2373,
        "start_time": "2023-05-15T12:29:53.219Z"
      },
      {
        "duration": 129,
        "start_time": "2023-05-15T12:29:59.953Z"
      },
      {
        "duration": 13,
        "start_time": "2023-05-15T12:30:00.329Z"
      },
      {
        "duration": 274,
        "start_time": "2023-05-15T12:30:43.723Z"
      },
      {
        "duration": 50,
        "start_time": "2023-05-15T12:31:16.577Z"
      },
      {
        "duration": 3614,
        "start_time": "2023-05-15T12:31:18.425Z"
      },
      {
        "duration": 19,
        "start_time": "2023-05-15T12:31:23.932Z"
      },
      {
        "duration": 26,
        "start_time": "2023-05-15T12:31:24.145Z"
      },
      {
        "duration": 6,
        "start_time": "2023-05-15T12:37:45.267Z"
      },
      {
        "duration": 3,
        "start_time": "2023-05-15T12:39:03.876Z"
      },
      {
        "duration": 6,
        "start_time": "2023-05-15T12:39:12.588Z"
      },
      {
        "duration": 3,
        "start_time": "2023-05-15T12:39:32.235Z"
      },
      {
        "duration": 3,
        "start_time": "2023-05-15T12:39:37.035Z"
      },
      {
        "duration": 6,
        "start_time": "2023-05-15T12:39:37.731Z"
      },
      {
        "duration": 2411,
        "start_time": "2023-05-15T12:40:07.517Z"
      },
      {
        "duration": 6,
        "start_time": "2023-05-15T12:40:12.051Z"
      },
      {
        "duration": 3,
        "start_time": "2023-05-15T12:40:25.660Z"
      },
      {
        "duration": 878,
        "start_time": "2023-05-15T12:40:26.387Z"
      },
      {
        "duration": 1619,
        "start_time": "2023-05-15T12:41:16.526Z"
      },
      {
        "duration": 968,
        "start_time": "2023-05-15T12:41:18.147Z"
      },
      {
        "duration": 16,
        "start_time": "2023-05-15T12:41:19.126Z"
      },
      {
        "duration": 20,
        "start_time": "2023-05-15T12:41:19.144Z"
      },
      {
        "duration": 57,
        "start_time": "2023-05-15T12:41:19.165Z"
      },
      {
        "duration": 14,
        "start_time": "2023-05-15T12:41:19.229Z"
      },
      {
        "duration": 8,
        "start_time": "2023-05-15T12:41:19.245Z"
      },
      {
        "duration": 2303,
        "start_time": "2023-05-15T12:41:19.255Z"
      },
      {
        "duration": 9,
        "start_time": "2023-05-15T12:41:21.560Z"
      },
      {
        "duration": 1892,
        "start_time": "2023-05-15T12:42:24.079Z"
      },
      {
        "duration": 857,
        "start_time": "2023-05-15T12:42:25.974Z"
      },
      {
        "duration": 12,
        "start_time": "2023-05-15T12:42:26.833Z"
      },
      {
        "duration": 23,
        "start_time": "2023-05-15T12:42:26.847Z"
      },
      {
        "duration": 44,
        "start_time": "2023-05-15T12:42:26.871Z"
      },
      {
        "duration": 17,
        "start_time": "2023-05-15T12:42:26.916Z"
      },
      {
        "duration": 16,
        "start_time": "2023-05-15T12:42:26.935Z"
      },
      {
        "duration": 2734,
        "start_time": "2023-05-15T12:42:26.953Z"
      },
      {
        "duration": 18,
        "start_time": "2023-05-15T12:42:29.689Z"
      },
      {
        "duration": 140,
        "start_time": "2023-05-15T12:44:17.619Z"
      },
      {
        "duration": 2207,
        "start_time": "2023-05-15T12:44:25.026Z"
      },
      {
        "duration": 5,
        "start_time": "2023-05-15T12:44:33.917Z"
      },
      {
        "duration": 11,
        "start_time": "2023-05-15T12:45:51.294Z"
      },
      {
        "duration": 4,
        "start_time": "2023-05-15T12:46:09.142Z"
      },
      {
        "duration": 10615,
        "start_time": "2023-05-15T12:46:50.494Z"
      },
      {
        "duration": 5,
        "start_time": "2023-05-15T12:53:31.928Z"
      },
      {
        "duration": 49,
        "start_time": "2023-05-15T12:55:32.187Z"
      },
      {
        "duration": 38,
        "start_time": "2023-05-15T12:55:51.015Z"
      },
      {
        "duration": 39,
        "start_time": "2023-05-15T12:57:14.863Z"
      },
      {
        "duration": 17,
        "start_time": "2023-05-15T12:57:48.020Z"
      },
      {
        "duration": 8305,
        "start_time": "2023-05-15T12:59:11.083Z"
      },
      {
        "duration": 6,
        "start_time": "2023-05-15T13:04:39.727Z"
      },
      {
        "duration": 86,
        "start_time": "2023-05-15T13:07:03.350Z"
      },
      {
        "duration": 58,
        "start_time": "2023-05-15T13:11:59.019Z"
      },
      {
        "duration": 253,
        "start_time": "2023-05-15T13:12:14.002Z"
      },
      {
        "duration": 179,
        "start_time": "2023-05-15T13:12:36.548Z"
      },
      {
        "duration": 78422,
        "start_time": "2023-05-15T13:13:48.705Z"
      },
      {
        "duration": 37156,
        "start_time": "2023-05-15T13:15:49.647Z"
      },
      {
        "duration": 303022,
        "start_time": "2023-05-15T13:26:47.100Z"
      },
      {
        "duration": 2060,
        "start_time": "2023-05-15T13:59:14.385Z"
      },
      {
        "duration": 962,
        "start_time": "2023-05-15T13:59:16.447Z"
      },
      {
        "duration": 14,
        "start_time": "2023-05-15T13:59:17.411Z"
      },
      {
        "duration": 31,
        "start_time": "2023-05-15T13:59:17.427Z"
      },
      {
        "duration": 27,
        "start_time": "2023-05-15T13:59:17.460Z"
      },
      {
        "duration": 17,
        "start_time": "2023-05-15T13:59:17.489Z"
      },
      {
        "duration": 10,
        "start_time": "2023-05-15T13:59:17.510Z"
      },
      {
        "duration": 2710,
        "start_time": "2023-05-15T13:59:17.522Z"
      },
      {
        "duration": 5,
        "start_time": "2023-05-15T13:59:20.234Z"
      },
      {
        "duration": 2951,
        "start_time": "2023-05-15T13:59:20.241Z"
      },
      {
        "duration": 11,
        "start_time": "2023-05-15T13:59:23.195Z"
      },
      {
        "duration": 16,
        "start_time": "2023-05-15T13:59:23.207Z"
      },
      {
        "duration": 9260,
        "start_time": "2023-05-15T13:59:23.225Z"
      },
      {
        "duration": 69,
        "start_time": "2023-05-15T13:59:32.486Z"
      },
      {
        "duration": 63,
        "start_time": "2023-05-15T13:59:32.557Z"
      },
      {
        "duration": 6885,
        "start_time": "2023-05-15T13:59:32.622Z"
      },
      {
        "duration": 35408,
        "start_time": "2023-05-15T13:59:39.509Z"
      },
      {
        "duration": 1540,
        "start_time": "2023-05-15T14:13:28.530Z"
      },
      {
        "duration": 931,
        "start_time": "2023-05-15T14:13:30.072Z"
      },
      {
        "duration": 15,
        "start_time": "2023-05-15T14:13:31.004Z"
      },
      {
        "duration": 69,
        "start_time": "2023-05-15T14:13:31.020Z"
      },
      {
        "duration": 74,
        "start_time": "2023-05-15T14:13:31.090Z"
      },
      {
        "duration": 51,
        "start_time": "2023-05-15T14:13:31.166Z"
      },
      {
        "duration": 53,
        "start_time": "2023-05-15T14:13:31.219Z"
      },
      {
        "duration": 2853,
        "start_time": "2023-05-15T14:13:31.274Z"
      },
      {
        "duration": 6,
        "start_time": "2023-05-15T14:13:34.129Z"
      },
      {
        "duration": 3096,
        "start_time": "2023-05-15T14:13:34.138Z"
      },
      {
        "duration": 4,
        "start_time": "2023-05-15T14:13:37.236Z"
      },
      {
        "duration": 13,
        "start_time": "2023-05-15T14:13:37.242Z"
      },
      {
        "duration": 64,
        "start_time": "2023-05-15T14:13:37.257Z"
      },
      {
        "duration": 3,
        "start_time": "2023-05-15T14:13:37.323Z"
      },
      {
        "duration": 6984,
        "start_time": "2023-05-15T14:13:37.327Z"
      },
      {
        "duration": 35109,
        "start_time": "2023-05-15T14:13:44.313Z"
      },
      {
        "duration": 399830,
        "start_time": "2023-05-15T14:14:19.424Z"
      },
      {
        "duration": 548,
        "start_time": "2023-05-15T14:24:34.750Z"
      },
      {
        "duration": 65,
        "start_time": "2023-05-15T14:24:39.758Z"
      },
      {
        "duration": 1774,
        "start_time": "2023-05-15T14:25:10.127Z"
      },
      {
        "duration": 2534,
        "start_time": "2023-05-15T14:25:13.411Z"
      },
      {
        "duration": 14,
        "start_time": "2023-05-15T14:25:15.947Z"
      },
      {
        "duration": 37,
        "start_time": "2023-05-15T14:25:15.963Z"
      },
      {
        "duration": 26,
        "start_time": "2023-05-15T14:25:16.008Z"
      },
      {
        "duration": 8,
        "start_time": "2023-05-15T14:25:16.036Z"
      },
      {
        "duration": 8,
        "start_time": "2023-05-15T14:25:16.045Z"
      },
      {
        "duration": 2859,
        "start_time": "2023-05-15T14:25:16.054Z"
      },
      {
        "duration": 5,
        "start_time": "2023-05-15T14:25:18.915Z"
      },
      {
        "duration": 3152,
        "start_time": "2023-05-15T14:25:18.922Z"
      },
      {
        "duration": 4,
        "start_time": "2023-05-15T14:25:22.076Z"
      },
      {
        "duration": 6,
        "start_time": "2023-05-15T14:25:32.231Z"
      },
      {
        "duration": 62,
        "start_time": "2023-05-15T14:25:33.766Z"
      },
      {
        "duration": 3,
        "start_time": "2023-05-15T14:25:37.847Z"
      },
      {
        "duration": 7030,
        "start_time": "2023-05-15T14:25:38.215Z"
      },
      {
        "duration": 1964,
        "start_time": "2023-05-15T14:29:42.310Z"
      },
      {
        "duration": 2558,
        "start_time": "2023-05-15T14:29:44.276Z"
      },
      {
        "duration": 13,
        "start_time": "2023-05-15T14:29:46.835Z"
      },
      {
        "duration": 33,
        "start_time": "2023-05-15T14:29:46.850Z"
      },
      {
        "duration": 59,
        "start_time": "2023-05-15T14:29:46.885Z"
      },
      {
        "duration": 11,
        "start_time": "2023-05-15T14:29:46.946Z"
      },
      {
        "duration": 12,
        "start_time": "2023-05-15T14:29:46.959Z"
      },
      {
        "duration": 5,
        "start_time": "2023-05-15T14:30:16.123Z"
      },
      {
        "duration": 3015,
        "start_time": "2023-05-15T14:30:20.369Z"
      },
      {
        "duration": 27,
        "start_time": "2023-05-15T14:30:23.386Z"
      },
      {
        "duration": 3150,
        "start_time": "2023-05-15T14:30:25.324Z"
      },
      {
        "duration": 4,
        "start_time": "2023-05-15T14:30:28.477Z"
      },
      {
        "duration": 33,
        "start_time": "2023-05-15T14:30:28.482Z"
      },
      {
        "duration": 55,
        "start_time": "2023-05-15T14:30:28.517Z"
      },
      {
        "duration": 6,
        "start_time": "2023-05-15T14:30:28.804Z"
      },
      {
        "duration": 7341,
        "start_time": "2023-05-15T14:30:29.403Z"
      },
      {
        "duration": 127,
        "start_time": "2023-05-15T14:30:42.337Z"
      },
      {
        "duration": 1938533,
        "start_time": "2023-05-15T14:30:52.522Z"
      },
      {
        "duration": 9,
        "start_time": "2023-05-15T15:15:27.685Z"
      },
      {
        "duration": 1415,
        "start_time": "2023-05-15T15:16:30.710Z"
      },
      {
        "duration": 883,
        "start_time": "2023-05-15T15:16:32.127Z"
      },
      {
        "duration": 13,
        "start_time": "2023-05-15T15:16:33.012Z"
      },
      {
        "duration": 46,
        "start_time": "2023-05-15T15:16:33.027Z"
      },
      {
        "duration": 33,
        "start_time": "2023-05-15T15:16:33.074Z"
      },
      {
        "duration": 16,
        "start_time": "2023-05-15T15:16:33.109Z"
      },
      {
        "duration": 9,
        "start_time": "2023-05-15T15:16:33.126Z"
      },
      {
        "duration": 2602,
        "start_time": "2023-05-15T15:16:33.137Z"
      },
      {
        "duration": 4,
        "start_time": "2023-05-15T15:16:35.741Z"
      },
      {
        "duration": 2897,
        "start_time": "2023-05-15T15:16:35.747Z"
      },
      {
        "duration": 5,
        "start_time": "2023-05-15T15:16:38.646Z"
      },
      {
        "duration": 7,
        "start_time": "2023-05-15T15:16:38.653Z"
      },
      {
        "duration": 63,
        "start_time": "2023-05-15T15:16:38.662Z"
      },
      {
        "duration": 3,
        "start_time": "2023-05-15T15:16:38.726Z"
      },
      {
        "duration": 6394,
        "start_time": "2023-05-15T15:16:38.730Z"
      },
      {
        "duration": 39884,
        "start_time": "2023-05-15T15:16:45.126Z"
      },
      {
        "duration": 358669,
        "start_time": "2023-05-15T15:17:25.012Z"
      },
      {
        "duration": 1884601,
        "start_time": "2023-05-15T15:23:23.683Z"
      },
      {
        "duration": 17,
        "start_time": "2023-05-15T15:54:48.286Z"
      },
      {
        "duration": 5427,
        "start_time": "2023-05-15T15:58:34.851Z"
      },
      {
        "duration": 196,
        "start_time": "2023-05-15T16:02:12.406Z"
      },
      {
        "duration": 54,
        "start_time": "2023-05-15T16:02:45.935Z"
      },
      {
        "duration": 14,
        "start_time": "2023-05-15T16:04:09.719Z"
      },
      {
        "duration": 25949,
        "start_time": "2023-05-15T16:04:26.920Z"
      },
      {
        "duration": 23497,
        "start_time": "2023-05-15T16:06:16.171Z"
      },
      {
        "duration": 9,
        "start_time": "2023-05-15T16:07:01.952Z"
      },
      {
        "duration": 24205,
        "start_time": "2023-05-15T16:08:15.394Z"
      },
      {
        "duration": 42537,
        "start_time": "2023-05-15T16:08:47.369Z"
      },
      {
        "duration": 12,
        "start_time": "2023-05-15T16:09:34.603Z"
      },
      {
        "duration": 13,
        "start_time": "2023-05-15T16:12:42.026Z"
      },
      {
        "duration": 1398,
        "start_time": "2023-05-15T16:13:05.691Z"
      },
      {
        "duration": 7732,
        "start_time": "2023-05-15T16:14:03.619Z"
      },
      {
        "duration": 1486,
        "start_time": "2023-05-15T16:14:15.931Z"
      },
      {
        "duration": 1531,
        "start_time": "2023-05-15T16:14:34.375Z"
      },
      {
        "duration": 6,
        "start_time": "2023-05-15T16:15:06.469Z"
      },
      {
        "duration": 1537,
        "start_time": "2023-05-15T16:15:13.228Z"
      },
      {
        "duration": 130,
        "start_time": "2023-05-15T16:17:15.844Z"
      },
      {
        "duration": 26,
        "start_time": "2023-05-15T16:23:09.191Z"
      },
      {
        "duration": 13445,
        "start_time": "2023-05-16T09:32:00.416Z"
      },
      {
        "duration": 3003,
        "start_time": "2023-05-16T09:32:53.007Z"
      },
      {
        "duration": 2544,
        "start_time": "2023-05-16T09:33:23.595Z"
      },
      {
        "duration": 18,
        "start_time": "2023-05-16T09:33:26.490Z"
      },
      {
        "duration": 41,
        "start_time": "2023-05-16T09:33:27.357Z"
      },
      {
        "duration": 34,
        "start_time": "2023-05-16T09:33:30.353Z"
      },
      {
        "duration": 8,
        "start_time": "2023-05-16T09:33:32.049Z"
      },
      {
        "duration": 4,
        "start_time": "2023-05-16T09:33:49.819Z"
      },
      {
        "duration": 1212,
        "start_time": "2023-05-16T09:33:53.307Z"
      },
      {
        "duration": 4,
        "start_time": "2023-05-16T09:34:43.778Z"
      },
      {
        "duration": 2765,
        "start_time": "2023-05-16T09:34:50.855Z"
      },
      {
        "duration": 2782,
        "start_time": "2023-05-16T09:35:00.953Z"
      },
      {
        "duration": 7,
        "start_time": "2023-05-16T09:35:04.459Z"
      },
      {
        "duration": 4,
        "start_time": "2023-05-16T09:37:28.621Z"
      },
      {
        "duration": 2399,
        "start_time": "2023-05-16T09:37:29.591Z"
      },
      {
        "duration": 7,
        "start_time": "2023-05-16T09:37:33.659Z"
      },
      {
        "duration": 201,
        "start_time": "2023-05-16T09:41:31.057Z"
      },
      {
        "duration": 624,
        "start_time": "2023-05-16T09:41:55.099Z"
      },
      {
        "duration": 23,
        "start_time": "2023-05-16T09:47:30.243Z"
      },
      {
        "duration": 22,
        "start_time": "2023-05-16T09:47:36.041Z"
      },
      {
        "duration": 5,
        "start_time": "2023-05-16T09:47:39.057Z"
      },
      {
        "duration": 40,
        "start_time": "2023-05-16T09:48:02.657Z"
      },
      {
        "duration": 517,
        "start_time": "2023-05-16T09:48:04.761Z"
      },
      {
        "duration": 6,
        "start_time": "2023-05-16T09:49:14.609Z"
      },
      {
        "duration": 47,
        "start_time": "2023-05-16T09:49:26.628Z"
      },
      {
        "duration": 6,
        "start_time": "2023-05-16T09:49:27.690Z"
      },
      {
        "duration": 484,
        "start_time": "2023-05-16T09:49:28.946Z"
      },
      {
        "duration": 503,
        "start_time": "2023-05-16T09:55:24.204Z"
      },
      {
        "duration": 530,
        "start_time": "2023-05-16T09:58:50.850Z"
      },
      {
        "duration": 39,
        "start_time": "2023-05-16T09:59:27.214Z"
      },
      {
        "duration": 37,
        "start_time": "2023-05-16T10:04:57.388Z"
      },
      {
        "duration": 6,
        "start_time": "2023-05-16T10:05:45.595Z"
      },
      {
        "duration": 5,
        "start_time": "2023-05-16T13:12:47.901Z"
      },
      {
        "duration": 50,
        "start_time": "2023-05-16T13:12:54.352Z"
      },
      {
        "duration": 4,
        "start_time": "2023-05-16T13:26:28.185Z"
      },
      {
        "duration": 57,
        "start_time": "2023-05-16T13:26:28.737Z"
      },
      {
        "duration": 34,
        "start_time": "2023-05-16T13:31:04.156Z"
      },
      {
        "duration": 17458,
        "start_time": "2023-05-16T13:32:10.797Z"
      },
      {
        "duration": 36,
        "start_time": "2023-05-16T13:32:34.097Z"
      },
      {
        "duration": 4,
        "start_time": "2023-05-16T13:32:57.315Z"
      },
      {
        "duration": 112188,
        "start_time": "2023-05-16T13:33:02.200Z"
      },
      {
        "duration": 8,
        "start_time": "2023-05-16T13:34:58.041Z"
      },
      {
        "duration": 8,
        "start_time": "2023-05-16T13:35:24.019Z"
      },
      {
        "duration": 68,
        "start_time": "2023-05-16T13:36:36.858Z"
      },
      {
        "duration": 60,
        "start_time": "2023-05-16T13:37:05.649Z"
      },
      {
        "duration": 21,
        "start_time": "2023-05-16T13:37:53.970Z"
      },
      {
        "duration": 51,
        "start_time": "2023-05-16T13:38:46.410Z"
      },
      {
        "duration": 8,
        "start_time": "2023-05-16T13:39:01.955Z"
      },
      {
        "duration": 18,
        "start_time": "2023-05-16T13:39:10.987Z"
      },
      {
        "duration": 124386,
        "start_time": "2023-05-16T13:51:44.610Z"
      },
      {
        "duration": 24,
        "start_time": "2023-05-16T13:54:57.965Z"
      },
      {
        "duration": 16188,
        "start_time": "2023-05-16T14:01:30.056Z"
      },
      {
        "duration": 3700,
        "start_time": "2023-05-16T14:01:46.266Z"
      },
      {
        "duration": 4,
        "start_time": "2023-05-16T14:02:00.852Z"
      },
      {
        "duration": 941,
        "start_time": "2023-05-16T14:02:02.017Z"
      },
      {
        "duration": 37,
        "start_time": "2023-05-16T14:02:02.960Z"
      },
      {
        "duration": 41,
        "start_time": "2023-05-16T14:02:05.417Z"
      },
      {
        "duration": 52,
        "start_time": "2023-05-16T14:02:05.754Z"
      },
      {
        "duration": 8,
        "start_time": "2023-05-16T14:02:11.913Z"
      },
      {
        "duration": 5,
        "start_time": "2023-05-16T14:02:15.309Z"
      },
      {
        "duration": 3710,
        "start_time": "2023-05-16T14:02:17.554Z"
      },
      {
        "duration": 25,
        "start_time": "2023-05-16T14:02:21.267Z"
      },
      {
        "duration": 1026,
        "start_time": "2023-05-16T14:02:29.813Z"
      },
      {
        "duration": 6,
        "start_time": "2023-05-16T14:02:46.105Z"
      },
      {
        "duration": 605,
        "start_time": "2023-05-16T14:02:50.252Z"
      },
      {
        "duration": 1560365,
        "start_time": "2023-05-16T14:03:19.525Z"
      },
      {
        "duration": 27,
        "start_time": "2023-05-16T14:29:24.441Z"
      },
      {
        "duration": 18413,
        "start_time": "2023-05-16T14:30:53.435Z"
      },
      {
        "duration": 4173,
        "start_time": "2023-05-16T14:31:11.853Z"
      },
      {
        "duration": 935,
        "start_time": "2023-05-16T14:31:19.308Z"
      },
      {
        "duration": 37,
        "start_time": "2023-05-16T14:31:20.246Z"
      },
      {
        "duration": 76,
        "start_time": "2023-05-16T14:31:20.293Z"
      },
      {
        "duration": 31,
        "start_time": "2023-05-16T14:31:23.111Z"
      },
      {
        "duration": 9,
        "start_time": "2023-05-16T14:31:23.545Z"
      },
      {
        "duration": 5,
        "start_time": "2023-05-16T14:31:25.970Z"
      },
      {
        "duration": 4907,
        "start_time": "2023-05-16T14:31:26.368Z"
      },
      {
        "duration": 10,
        "start_time": "2023-05-16T14:31:31.277Z"
      },
      {
        "duration": 5,
        "start_time": "2023-05-16T14:32:01.306Z"
      },
      {
        "duration": 1546593,
        "start_time": "2023-05-16T14:32:06.138Z"
      },
      {
        "duration": 13,
        "start_time": "2023-05-16T14:59:12.246Z"
      },
      {
        "duration": 17,
        "start_time": "2023-05-16T14:59:31.651Z"
      },
      {
        "duration": 6,
        "start_time": "2023-05-16T15:00:48.105Z"
      },
      {
        "duration": 63,
        "start_time": "2023-05-16T15:00:49.454Z"
      },
      {
        "duration": 3,
        "start_time": "2023-05-16T15:00:52.343Z"
      },
      {
        "duration": 9529,
        "start_time": "2023-05-16T15:00:53.946Z"
      },
      {
        "duration": 4,
        "start_time": "2023-05-16T17:12:38.724Z"
      },
      {
        "duration": 51,
        "start_time": "2023-05-16T17:12:40.596Z"
      },
      {
        "duration": 16,
        "start_time": "2023-05-16T17:12:55.897Z"
      },
      {
        "duration": 8524,
        "start_time": "2023-05-16T17:12:57.411Z"
      },
      {
        "duration": 50112,
        "start_time": "2023-05-16T17:13:09.103Z"
      },
      {
        "duration": 453,
        "start_time": "2023-05-16T17:14:34.042Z"
      },
      {
        "duration": 64,
        "start_time": "2023-05-16T17:15:07.407Z"
      },
      {
        "duration": 50983,
        "start_time": "2023-05-16T17:15:21.940Z"
      },
      {
        "duration": 471901,
        "start_time": "2023-05-16T17:16:49.020Z"
      },
      {
        "duration": 2129675,
        "start_time": "2023-05-16T17:24:47.354Z"
      },
      {
        "duration": 12,
        "start_time": "2023-05-16T18:07:05.922Z"
      },
      {
        "duration": 1740,
        "start_time": "2023-05-16T18:07:08.214Z"
      },
      {
        "duration": 24,
        "start_time": "2023-05-16T18:07:13.300Z"
      }
    ],
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Содержание",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "302.391px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Проект для «Викишоп»",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span><ul class=\"toc-item\"><li><span><a href=\"#Первичный-анализ-данных\" data-toc-modified-id=\"Первичный-анализ-данных-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Первичный анализ данных</a></span></li><li><span><a href=\"#Очистка-текста\" data-toc-modified-id=\"Очистка-текста-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Очистка текста</a></span></li><li><span><a href=\"#Лемматизация-текста\" data-toc-modified-id=\"Лемматизация-текста-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Лемматизация текста</a></span></li></ul></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#Оценка-важности-слова-(TF-IDF)\" data-toc-modified-id=\"Оценка-важности-слова-(TF-IDF)-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Оценка важности слова (TF-IDF)</a></span></li><li><span><a href=\"#Деление-данных-на-выборки\" data-toc-modified-id=\"Деление-данных-на-выборки-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Деление данных на выборки</a></span></li><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>TF-IDF</a></span></li><li><span><a href=\"#Логистическая-регрессия\" data-toc-modified-id=\"Логистическая-регрессия-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Логистическая регрессия</a></span></li><li><span><a href=\"#Модель-Random-Forest\" data-toc-modified-id=\"Модель-Random-Forest-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Модель Random Forest</a></span></li><li><span><a href=\"#Модель-Decision-Tree\" data-toc-modified-id=\"Модель-Decision-Tree-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Модель Decision Tree</a></span></li><li><span><a href=\"#Сравнение-моделей\" data-toc-modified-id=\"Сравнение-моделей-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Сравнение моделей</a></span></li></ul></li><li><span><a href=\"#Тестирование\" data-toc-modified-id=\"Тестирование-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Тестирование</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>",
      "metadata": {
        "toc": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n\nОбучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n\nПостройте модель со значением метрики качества *F1* не меньше 0.75. \n\n**Инструкция по выполнению проекта**\n\n1. Загрузите и подготовьте данные.\n2. Обучите разные модели. \n3. Сделайте выводы.\n\nДля выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n\n**Описание данных**\n\nДанные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Подготовка",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Импортируем все необходимые библиотеки",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\n\nimport pandas as pd\n\nimport re\n\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords as nltk_stopwords\n\nimport sys\n!{sys.executable} -m pip install spacy\n!{sys.executable} -m spacy download en\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nRANDOM = 12345",
      "metadata": {
        "scrolled": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n\n[nltk_data]   Package stopwords is already up-to-date!\n"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Requirement already satisfied: spacy in /opt/conda/lib/python3.9/site-packages (3.2.0)\n\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from spacy) (3.0.1)\n\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (3.3.0)\n\nRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.9/site-packages (from spacy) (0.10.1)\n\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (2.25.1)\n\nRequirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (0.4.2)\n\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.9/site-packages (from spacy) (2.0.8)\n\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /opt/conda/lib/python3.9/site-packages (from spacy) (3.0.10)\n\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (1.21.1)\n\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.9/site-packages (from spacy) (1.8.2)\n\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (1.0.3)\n\nRequirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.9/site-packages (from spacy) (0.6.2)\n\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from spacy) (49.6.0.post20210108)\n\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (1.0.8)\n\nRequirement already satisfied: thinc<8.1.0,>=8.0.12 in /opt/conda/lib/python3.9/site-packages (from spacy) (8.0.17)\n\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (0.7.8)\n\nRequirement already satisfied: srsly<3.0.0,>=2.4.1 in /opt/conda/lib/python3.9/site-packages (from spacy) (2.4.4)\n\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy) (3.0.7)\n\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy) (2.0.6)\n\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (21.3)\n\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (4.61.2)\n\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->spacy) (2.4.7)\n\nRequirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/conda/lib/python3.9/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (4.3.0)\n\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.6)\n\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.3)\n\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->spacy) (2.1.1)\n\n\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n\nfull pipeline package name 'en_core_web_sm' instead.\u001b[0m\n\nCollecting en-core-web-sm==3.2.0\n\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n\n\u001b[K     |████████████████████████████████| 13.9 MB 7.5 MB/s eta 0:00:01\n\n\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from en-core-web-sm==3.2.0) (3.2.0)\n\nRequirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.2)\n\nRequirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.2)\n\nRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.10.1)\n\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.3)\n\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.8)\n\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.25.1)\n\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (49.6.0.post20210108)\n\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.8)\n\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.7)\n\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.1)\n\nRequirement already satisfied: thinc<8.1.0,>=8.0.12 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.17)\n\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.1)\n\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.10)\n\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.61.2)\n\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.8)\n\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n\nRequirement already satisfied: srsly<3.0.0,>=2.4.1 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.4)\n\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.7)\n\nRequirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/conda/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.3.0)\n\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.6)\n\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.0.0)\n\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2022.6.15)\n\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.1.3)\n\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.1.1)\n\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n\nYou can now load the package via spacy.load('en_core_web_sm')\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "from tqdm import tqdm\nimport spacy",
      "metadata": {},
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Все необходимые для дальнейшей работы библиотеки импортированы. Приступим к рассмотрению данных.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Первичный анализ данных",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Откроем наш датасет и сохраним его в переменной df. Затем выведем на экран первые 5 строк датасета, а также информацию о нем с помощью метода info(). А также сразу проверим датасет на предмет пропусков.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df = pd.read_csv('/datasets/toxic_comments.csv')",
      "metadata": {},
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "df.head()",
      "metadata": {},
      "execution_count": 4,
      "outputs": [
        {
          "execution_count": 4,
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>toxic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                                               text  toxic\n",
              "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
              "1           1  D'aww! He matches this background colour I'm s...      0\n",
              "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
              "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
              "4           4  You, sir, are my hero. Any chance you remember...      0"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "df.info()",
      "metadata": {},
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "<class 'pandas.core.frame.DataFrame'>\n\nRangeIndex: 159292 entries, 0 to 159291\n\nData columns (total 3 columns):\n\n #   Column      Non-Null Count   Dtype \n\n---  ------      --------------   ----- \n\n 0   Unnamed: 0  159292 non-null  int64 \n\n 1   text        159292 non-null  object\n\n 2   toxic       159292 non-null  int64 \n\ndtypes: int64(2), object(1)\n\nmemory usage: 3.6+ MB\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "df.isna().sum()",
      "metadata": {},
      "execution_count": 6,
      "outputs": [
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unnamed: 0    0\n",
              "text          0\n",
              "toxic         0\n",
              "dtype: int64"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "Как мы видим из полученного - наш датасет состоит из 3-х колонок (номер, текст комментария и оценка токсичности (0 или 1). Следовательно, перед нами задача классификации - нам необходимо обучить модель, которая будет определять токсичный комментарий или нет (0 или 1). Также в датасете отсутствуют пропуски. Выведем на экран колонку \"Текст\", чтобы посмотреть в каком состоянии находятся в нем данные.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df.text",
      "metadata": {
        "scrolled": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         Explanation\\nWhy the edits made under my usern...\n",
              "1         D'aww! He matches this background colour I'm s...\n",
              "2         Hey man, I'm really not trying to edit war. It...\n",
              "3         \"\\nMore\\nI can't make any real suggestions on ...\n",
              "4         You, sir, are my hero. Any chance you remember...\n",
              "                                ...                        \n",
              "159287    \":::::And for the second time of asking, when ...\n",
              "159288    You should be ashamed of yourself \\n\\nThat is ...\n",
              "159289    Spitzer \\n\\nUmm, theres no actual article for ...\n",
              "159290    And it looks like it was actually you who put ...\n",
              "159291    \"\\nAnd ... I really don't think you understand...\n",
              "Name: text, Length: 159292, dtype: object"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "Мы видим, что текст на английском языке, имеются различные разделители, не ровные отступы, рандомные символы. Ото всего этого необходимо избавиться перед тем, как дальше работать с текстом",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Очистка текста",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "От лишних символов текст очистят регулярные выражения. Это инструмент для поиска слова или числа по шаблону. Он определяет, из каких частей состоит строка и какие в них символы. Воспользуемся функцией re.sub(), в которую передадим символы английского алфавита (заглавные и строчные), которые нужно заменить на пробелы. Затем в такую же функцию передадим разделители, которые встречаются в наших комментариях, и также заменим их на пробелы. Методом join() объединим получившиеся элементы в строки. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def clear_text(text):\n    text = re.sub(r'[^a-zA-Z ]', ' ', text)\n    text = re.sub(r'(?:\\n|\\r)', ' ', text)\n    text = text.lower()\n    return \" \".join(text.split())",
      "metadata": {},
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Теперь заменим столбец \"Текст\" на столбец \"Текст\" очищенный и посмотрим что получилось.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df['text'] = df['text'].apply(clear_text)",
      "metadata": {},
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "df.text",
      "metadata": {
        "scrolled": true
      },
      "execution_count": 10,
      "outputs": [
        {
          "execution_count": 10,
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         explanation why the edits made under my userna...\n",
              "1         d aww he matches this background colour i m se...\n",
              "2         hey man i m really not trying to edit war it s...\n",
              "3         more i can t make any real suggestions on impr...\n",
              "4         you sir are my hero any chance you remember wh...\n",
              "                                ...                        \n",
              "159287    and for the second time of asking when your vi...\n",
              "159288    you should be ashamed of yourself that is a ho...\n",
              "159289    spitzer umm theres no actual article for prost...\n",
              "159290    and it looks like it was actually you who put ...\n",
              "159291    and i really don t think you understand i came...\n",
              "Name: text, Length: 159292, dtype: object"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "Как мы видим - лишние символы, отступы и пробелы убрались. Текст выглядит пригодным для дальнейшей работы с ним. Перейдем к лемматизации.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Лемматизация текста",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Воспользуемся функцией spaCy для проведения лемматизации текста, т.е. преобразования слов в леммы (исходные слова).\nЗаранее мы загрузили библиотеку spaCy, а также словарь текста на английском - т.к. у нас комментарии на английском языке. \n\nСоздадим переменную corpus, в которую передадим значения столбца \"текст\" нашего датасета, а затем напишем функцию лемматизации, которая возвращает слову исходную форму. Затем пройдемся списком по переменной corpus, чтобы провести полную лемматизацию.\n\nВ конце заменим исходный столбец \"текст\" лемматизированным.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "corpus = df['text'].values",
      "metadata": {},
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\ndef lemmatize(text):\n    doc = nlp(text)\n    lemm_text = \" \".join([token.lemma_ for token in doc])\n    return lemm_text\n\nlemm=[]\nfor i in tqdm(range(len(corpus))):\n    lemm.append(lemmatize(corpus[i]))\n\n    \ndf['text'] = pd.Series(lemm, index=df.index)",
      "metadata": {
        "scrolled": true
      },
      "execution_count": 12,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "100%|██████████| 159292/159292 [25:45<00:00, 103.05it/s]\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "Посмотрим что получилось.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df",
      "metadata": {
        "scrolled": true
      },
      "execution_count": 14,
      "outputs": [
        {
          "execution_count": 14,
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>toxic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>explanation why the edit make under my usernam...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>d aww he match this background colour I m seem...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>hey man I m really not try to edit war it s ju...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>more I can t make any real suggestion on impro...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>you sir be my hero any chance you remember wha...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159287</th>\n",
              "      <td>159446</td>\n",
              "      <td>and for the second time of ask when your view ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159288</th>\n",
              "      <td>159447</td>\n",
              "      <td>you should be ashamed of yourself that be a ho...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159289</th>\n",
              "      <td>159448</td>\n",
              "      <td>spitzer umm there s no actual article for pros...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159290</th>\n",
              "      <td>159449</td>\n",
              "      <td>and it look like it be actually you who put on...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159291</th>\n",
              "      <td>159450</td>\n",
              "      <td>and I really don t think you understand I come...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>159292 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Unnamed: 0                                               text  toxic\n",
              "0                0  explanation why the edit make under my usernam...      0\n",
              "1                1  d aww he match this background colour I m seem...      0\n",
              "2                2  hey man I m really not try to edit war it s ju...      0\n",
              "3                3  more I can t make any real suggestion on impro...      0\n",
              "4                4  you sir be my hero any chance you remember wha...      0\n",
              "...            ...                                                ...    ...\n",
              "159287      159446  and for the second time of ask when your view ...      0\n",
              "159288      159447  you should be ashamed of yourself that be a ho...      0\n",
              "159289      159448  spitzer umm there s no actual article for pros...      0\n",
              "159290      159449  and it look like it be actually you who put on...      0\n",
              "159291      159450  and I really don t think you understand I come...      0\n",
              "\n",
              "[159292 rows x 3 columns]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "Текст лемматизирован.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Таким образом, мы проанализировали исходные данные - оказалось, что они чистые. Затем мы перешли к обработке самих текстов комментариев - для начала мы избавились от ненужых пробелов, символов-разделителей, случайных символов. А затем провели лемматизацию текстов, т.е. привели слова к изначальной форме. \n\nКак и говорилось ранее - данная задача классификации, а следовательно, будем использовать для ее решения модели классификации. Приступим.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Обучение",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Оценка важности слова (TF-IDF)",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Непосредственно перед подготовкой к обучению моделей и самим обучением, выполним еще один важный шаг - сделаем оценку важности слова (TF-IDF векторизацию). TF отвечает за количество упоминаний слова в отдельном тексте, а IDF отражает частоту его употребления во всём корпусе. Для начала загрузим словарь \"стоп-слов\" на английском языке из библиотеки nltk.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "stopwords = set(nltk_stopwords.words('english'))",
      "metadata": {},
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Теперь данные необходимо разделить на выборки.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Деление данных на выборки",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "В нашей задачи целевой признак - это колонка \"токсичность\", а сопутствующие - все остальные колонки (в частности у нас это только колонка \"текст\"). Разделим данные на 2 выборки - тренировочную и тестовую в пропорции 75/25, а при обучении моделей будем использовать кросс-валидацию. После выведем на экран размеры выборок.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "X = df['text']\ny = df['toxic'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = RANDOM)\n\nprint(X_train.shape, y_train.shape) \nprint(X_test.shape, y_test.shape)",
      "metadata": {},
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "(119469,) (119469,)\n\n(39823,) (39823,)\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "Данные разделены на выборки. Продолжим заниматься TF-IDF векторизацией.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### TF-IDF",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Мы в первую очередь разделили данные, так как если данные уже разделены на обучающую и тестовую выборки, функцию fit_tramsform() необходимо запускать только на обучающей выборке. На тестовой мы запустим только функцию transform(). Иначе тестирование будет нечестным: в модели будут учтены частоты слов из тестовой выборки.\n\nСоздадим счетчик, указав в нем стоп слова.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "count_tf_idf = TfidfVectorizer(stop_words=stopwords) ",
      "metadata": {},
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Теперь воспользуемся, как говорилось выше, функцией fit_transform() на тренировочной выборке и функцией transform() на тестовой. Сохраним полученные результаты в новые переменные.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "tf_idf_train = count_tf_idf.fit_transform(X_train)\ntf_idf_test = count_tf_idf.transform(X_test)",
      "metadata": {},
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Подготовка к обучению моделей завершена.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Для решения поставленной задачи воспользуемся 3-мя моделями:\n- Логистическая регрессия\n- Случайный лес\n- Дерево решений\n\nЛучшую модель будем определять по метрике 'f1', которая по условию задачи не должна быть меньше 0,75. Приступим.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Логистическая регрессия",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "pipeline = Pipeline([(('model', LogisticRegression(random_state=RANDOM, solver='liblinear', max_iter=300)))])\n\nparam_grid = [{'model__penalty' : ['l1', 'l2']}]\nglr = GridSearchCV(pipeline, param_grid=param_grid, scoring='f1', cv=5, verbose=True, n_jobs=-1)\nglr.fit(tf_idf_train, y_train)\nbest_params_lr = glr.best_params_\nbest_score_lr = glr.best_score_\nprint(\"Лучшие гиперпараметры для модели:\")\nprint()\nprint(best_params_lr)\nprint()\nprint(\"Лучшая метрика F1 с лучшими гиперпараметрами:\")\nprint()\nprint(best_score_lr)",
      "metadata": {},
      "execution_count": 26,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n\nЛучшие гиперпараметры для модели:\n\n\n\n{'model__penalty': 'l1'}\n\n\n\nЛучшая метрика F1 с лучшими гиперпараметрами:\n\n\n\n0.7649342813372237\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### Модель Random Forest",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "model = RandomForestClassifier(random_state=RANDOM, n_jobs=-1) \nest = [x for x in range(10, 200, 50)]\nparams_rf = [{'n_estimators': est,\n              'max_depth':[2, 10]}]\n\ngrf = GridSearchCV(model, params_rf, scoring='f1', cv=5, verbose=True)\ngrf.fit(tf_idf_train, y_train)\nbest_params_rf = grf.best_params_\nbest_score_rf = grf.best_score_\nprint(\"Лучшие гиперпараметры для модели:\")\nprint()\nprint(best_params_rf)\nprint()\nprint(\"Лучшая метрика F1 с лучшими гиперпараметрами:\")\nprint()\nprint(best_score_rf)\n",
      "metadata": {
        "scrolled": true
      },
      "execution_count": 27,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n\nЛучшие гиперпараметры для модели:\n\n\n\n{'max_depth': 10, 'n_estimators': 10}\n\n\n\nЛучшая метрика F1 с лучшими гиперпараметрами:\n\n\n\n0.0004939626969726234\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### Модель Decision Tree",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "model = DecisionTreeClassifier(random_state=RANDOM) \ndepth = [None] + [i for i in range(2, 10)]\nparams = [{'max_depth':depth}]\n\ngdt = GridSearchCV(model, params, scoring='f1', cv=5, verbose=True,  n_jobs=-1)\ngdt.fit(tf_idf_train, y_train)\nbest_params_dt = gdt.best_params_\nbest_score_dt = gdt.best_score_\nprint(\"Лучшие гиперпараметры для модели:\")\nprint()\nprint(best_params_dt)\nprint()\nprint(\"Лучшая метрика F1 с лучшими гиперпараметрами:\")\nprint()\nprint(best_score_dt)",
      "metadata": {
        "scrolled": true
      },
      "execution_count": 28,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n\nЛучшие гиперпараметры для модели:\n\n\n\n{'max_depth': None}\n\n\n\nЛучшая метрика F1 с лучшими гиперпараметрами:\n\n\n\n0.7126063136009069\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### Сравнение моделей",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Сравним получившиеся результаты в таблице.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "index = ['LogisticRegression',\n         'RandomForestClassifier',\n         'DecisionTreeClassifier']\nscores = {'Метрика F1 на тренировочной выборке с кросс-валидацией':[best_score_lr,\n                                                                    best_score_rf,\n                                                                    best_score_dt]}\n\nres = pd.DataFrame(data=scores, index=index)\nres",
      "metadata": {
        "scrolled": true
      },
      "execution_count": 29,
      "outputs": [
        {
          "execution_count": 29,
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Метрика F1 на тренировочной выборке с кросс-валидацией</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>LogisticRegression</th>\n",
              "      <td>0.764934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RandomForestClassifier</th>\n",
              "      <td>0.000494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTreeClassifier</th>\n",
              "      <td>0.712606</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        Метрика F1 на тренировочной выборке с кросс-валидацией\n",
              "LogisticRegression                                               0.764934     \n",
              "RandomForestClassifier                                           0.000494     \n",
              "DecisionTreeClassifier                                           0.712606     "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "Наибольшая метрика F1 оказалась у модели Логистической Регресии - 0,76. Ее мы и проверим на тестовой выборке.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Тестирование",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Протестируем нашу модель с наилучшей метрикой F1.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "model = LogisticRegression(random_state=RANDOM, penalty = 'l1', solver='liblinear', max_iter=300)\nmodel.fit(tf_idf_train, y_train)\npreds = model.predict(tf_idf_test)\nprint('Метрика F1 на тестовой выборке составила:', f1_score(y_test, preds))",
      "metadata": {},
      "execution_count": 30,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Метрика F1 на тестовой выборке составила: 0.7894883980989656\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "Проверим модель на адекватность - для этого сравним ее с константной (dummy) моделью.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\ndummy_clf.fit(tf_idf_train, y_train)\ndummy_pred = dummy_clf.predict(tf_idf_test)\ndummy_f1 = f1_score(y_test, dummy_pred)\nprint('Метрика F1 для константной модели на тестовой составляет:', \n      dummy_f1)",
      "metadata": {},
      "execution_count": 31,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Метрика F1 для константной модели на тестовой составляет: 0.0\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "Из этого можно сделать, что наша модель адекватна.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Выводы",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Перед нами стояла задача по обучению модели классифицировать комментарии интернет-магазина \"Викишоп\" на позитивные и негативные. Так как у магазина появился новый сервис - пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах, т.е. клиенты предлагают свои правки и комментируют изменения других. Магазину нужен был инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. Для этого в нашем распоряжении был набор данных с разметкой о токсичности правок. ТО есть нам надо получить на выходе токсичный комментарий или нет (0 или 1), а следовательно, перед нами стояла задача классификации.\n\nТакже стоило учесть, что метрика качества F1 должна быть не меньше 0.75.\n\nМы выгрузили интересующий нас датасет и провели его предварительную обработку. Мы проанализировали исходные данные - оказалось, что они чистые. Затем мы перешли к обработке самих текстов комментариев - для начала мы избавились от ненужых пробелов, символов-разделителей, случайных символов. А затем провели лемматизацию текстов, т.е. привели слова к изначальной форме.\n\nЗатем, для дальнейшего оубчения нами было выбрано 3 модели классификации: \n- Логистическая регрессия\n- Случайный лес\n- Дерево решений\n\nПеред подготовкой к обучению моделей и самим обучением, выполним еще один важный шаг - сделаем оценку важности слова (TF-IDF векторизацию). Затем мы разделили данные на 2 выборки - тренировочную и тестовую в пропорции 75/25, а при обучении моделей использовали кросс-валидацию.\n\nПосле разделения данных мы использовали функцию fit_tramsform() только на обучающей выборке. На тестовой мы запустили только функцию transform() - иначе тестирование было бы нечестным: в модели были бы учтены частоты слов из тестовой выборки.\n\nЗатем мы выделили целевой и сопутствующие признаки: целевой признак - это колонка \"токсичность\", а сопутствующие - все остальные колонки (в частности у нас это только колонка \"текст\").\n\nДалее мы приступили непосредственно к обучению моделей на тренировочной выборке. Для поиска лучших гиперпараметров была использована функция GridSearchCV(). Итого, после обучения мы получили следующие метрики качества F1:\n- Логистическая регрессия 0,764\n- Случайный лес 0,0004\n- Дерево решений 0,712\n\nКак лучшая модель, была выбрана модель Логистической регрессии. Ее мы и протестировали.\n\nНа тестовой выборке метрика качества F1 равна 0,789, что входит в допустимый предел. Также мы проверили модель на адекватность с помощью константной модели - наша модель оказалась адекватна.\n\nТаким образом, для решения стоящей перед заказчиком задачи, лучше всего подходит модель Логистической регрессии с подобранными нами гиперпараметрами.",
      "metadata": {}
    }
  ]
}